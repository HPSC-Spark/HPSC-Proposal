\documentclass{article}
\usepackage{natbib}

%%%
%
% I wrote outline in the comments below to helpfully guide us
% but obviously feel free to modify/stray from it as you see fit
%
%%%

\title{High-Performance Data Analysis on Janus using Apache Spark}
\author{Nick Vanderweit \\
        Ning Gao \\
        Anitha Ganesha \\
        Michael Kasper}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section*{Introduction}
MapReduce \citep{dean-mapreduce} is a programming paradigm for performing highly
parallel tasks on large sets of data. Designed at Google to provide an
abstraction for interacting with raw data at scale, MapReduce is based on
a master/slave model, where the workers are assigned tasks as they become idle.

MapReduce is named after the two kinds of tasks that workers can be assigned.
A \emph{Map} is a user-defined function of type
$(k_1 \times v_1) \rightarrow list \: (k_2 \times v_2)$.
That is, it takes a key-value pair and produces a list of
intermediate key-value pairs. \emph{Reduce} has type
$(k_1 \times list \: v_1) \rightarrow list \: v_2$.\footnote{
These tasks are inspired by the \emph{map} and \emph{fold}
higher-order functions from functional programming, but are not identical.}
Reduce tasks are responsible for distilling a set of values associated with
a key to a smaller set, often containing just one value.

Despite its popularity, the MapReduce architecture has some significant
limitations \citep{lee-survey}. For one, Reduce tasks can only be assigned after
all Map tasks have been completed, meaning that nodes will idle even when there
is work remaining. Another major performance limitation, which will be central
to the motivation behind this project, is the architecture's reliance on a
distributed filesystem. This limits the performance of iterative computations
because every Map or Reduce task needs to read its input from storage and write
its output back.

\subsection*{Apache Spark}
Apache Spark \citep{zaharia} is designed to remove this bottleneck in order to
increase the performance of iterative computations. Spark was introduced by
AMPLab at University of California Berkeley. Its storage model is
based around \emph{Resilient Distributed Datasets}, or
RDDs. \citep{zaharia_rdd}

An RDD is processed in parallel by being split into \emph{partitions}, which
are shared between nodes. The dataset is accessed through a handle, which
represents the RDD as a \emph{lineage} of transformations on persistent data.
This allows RDDs to be recovered in the event of node failure, without writing
all intermediate data to distributed storage. Spark is written in Scala,
a statically-typed functional JVM language, and allows interaction with datasets
using the Scala collections API.

% what is map-reduce, history, applications (1 Ref Paper) 
%   - origin, date, creators, original purpose
%   - growing popularity, expanding applications
%   - common technologies/tools, Hadoop
% Janus description, why Janus can't use Hadoop (1 Ref Paper) 
%       \citep{tufo}
%   - what, where is it
%   - architecture, hardware: number of nodes, procs, storage
%   - focus on disk-less, common for HPC?
%   - has not yet employed MR technologies
%   - problem with Hadoop and disk-less

\subsection*{Janus Super Computer}
Janus Super computer mainly consists of 1,368 computing nodes, with  12 cores
each that adds upto a total of 16,416 cores. Each node consists of two hex core
2.8Mhz Intel westmere processor. Each core contain 2GB of Ram with a total of
24GB of RAM per node. It  contains 32TB high performance storage with lustre
filesystem. 

Since Janus is diskless, it is not possible to install Hadoop on Janus.
However, Spark is an in-memory based cluster computing system which does not
rely on disk data for every iteration and hence it could be easily deployed on
Janus super computer and evaluated.


% what is spark, differs from hadoop 
% Spark\citep{zaharia_rdd}, known for its peformance in iterative and
% interactive tasks, was introduced by AmpLab group in University of California
% Berkeley. One of the best features that Spark supports is fault-tolerance.
% With hundreds or even thousands of machines in one cluster, the failure rate
% for one computer is not negligible. Spark is able to recover from failed
% machines by rebuilding the lost data from the lineage information of the RDD.
% On the other hand, Spark is so expressible that it not only supports mapreduce
% model and also Pregel, DryadLINQ, which means the program you run on mapreduce
% can be modified to work on Spark.
% Spark\citep{zaharia} \citep{zaharia_rdd}
%   - relationship with Scala (statically typed HL programming language for JVM)
%   - spark another MR solution, but all in-memory, year created
%   - solves the disk-less problem, can run on Janus
%   - other contrasts with Hadoop, preferred domains
%         - iterative, interactive, beyond MapReduce

%   - what is shark (do we want to use Shark in our project?)
%       - shark comes with its own set of benchmark queries

% Provides easy SQL programming interface to Spark's deep analysis tools.
% Shark\citep{engle}

\section*{Existing Performance Evaluations }
Despite being a relatively new technology, a considerable amount of performance
evaluations has been performed on the wide array of MapReduce solutions.
In Google's first paper introduction MapReduce, they provided a number of
example tasks in which MapReduce can be applied. A few of those listed were
word count, distributed grep, counting URL access frequency, \& reverse web-link
graph \citep{dean-mapreduce}. These examples have often been implemented for
benchmarking other MapReduce technologies and gives us a initial point in which
to start our evaluation of Spark on Janus.

%% Assuming Hadoop reference above
Hadoop, being one of the more popular MapReduce frameworks, has been in a
number of different studies. For this reason (and its large contrast in how
it utilizes memory and disk storage), Hadoop will make for an interesting
comparison with Spark.

% existing spark evaluation
Most of the benchmark programs run on Spark system were iterative and
interactive tasks. For instance, the initial work on RDDs \citep{zaharia_rdd}
used iterative machine learning algorithms, as well as PageRank with public
Wikipedia data. Based on the response time for interactive queries on
this data, Spark was shown to be over an order of magnitude faster than
on-disk technologies.
Andrew Pavlo et al. \cite{andrew} compared SQL performance between
MapReduce and DBMS system and Amplab \citep{amplab_bench}, inspired by the
benchmark in the previous paper and then conducted benchmarks on Shark,
Hive\citep{ashish}, Impala and Redshift.
% performance of mapreduce: an in-depth study
% written in Java
% Hadoop\citep{jiang}


As Spark is a newer technologies, it does not have the same quantity of
performance evaluations as Hadoop. But there still has been a significant
amount of conducted research on Spark (and the related Shark) that will gives
some initial insight and its performance that will aid us in our research.

% outperforms Hadoop by 10x in iterative machine learning jobs
% queries 39GB dataset with sub-second response time
% \citep{zaharia}

% Resilient Distributed Datasets (RDDs)
%     read only collection of objects
%     partitioned across a set of machines
%     data need not exist in physical storage

% Parallel Operations on these datasets
% \citep{zaharia}


%               "Performance and Scalability of Broadcast in Spark"
%               determined broadcast limited scalability
%               compares performance of Spark vs Hadoop on Logistic Reg prob
%               times during first and second iterations
%               ratio: computation vs broadcast times during an iteration
% Broadcast\citep{chowdhury}
% load into memory, read only happens once, overhead of creating workers reduce

As we will be running Spark with Shark, we will also briefly touch on the
reported performance of Shark. When compared to Hive (Hadoop's respective
SQL interface) we see that the run-times for the first iteration are comparable,
(when data need to be loaded into memory). But Shark is shown to be 10 orders
of magnitude faster during subsequent iterations of an algorithms\citep{engle}.
Even if this is not a pure comparison of the Hadoop and Spark technologies, we
can still glean some additional insights as to where each technology performs
best.


% introduction to shark
%   SQL interface for spark
%   abstracts coding mapreduce functions
%   - shark study results
% Shark\citep{engle}

\section*{Proposal}
% room for further research
%   - what are questions we can still try to answer
%   - how does Janus make this a special case
%       - make very Janus specific

\subsection*{Setup/Profiling}

install spark/scala
- create group, access to all group members
- setup script to start master
install benchmark tools configure
initial benchmarking mark on in-master-memory data
initial benchmarking mark on from scratch
fit all in memory of single core
- monitor cache performance
exceeding memory of single core
- monitor cache performance
load balancing data skew
install controlled data partitioning extension
- benchmark before and after

data:
stack overflow data public
- distributed grep
- itertive application linear regresssion

- iterative program: clustering
- iterative program: classification

that can segway into longer project if we have time

shark setup if time permits


how do you benchmark MPI programs?
can we use the same for spark?




% do same grep test as google & Hadoop

%get spark running on janus
%monitoring - ganglia
%make it easy for using
%administrative side, getting it to run

%might look into shark if need be

% profile performance of spark on Janus 
% existing technologies for MR
% possibility of adapting these benchmarks to run on Janus
% find/address weak points/Janus-specific problems 
% possible areas to focus on during evaluation:
%   - too big for memory (benchmarking on this specific problem) 
%   - data shuffling work \citep{jiaxing}
%   - network congestion 
%   - iterative, interactive analysis
%   - load balancing
%   - fault tolerance
%	- data skew \citep{qifa} \citep{ganesh}
% create models that can be used both others in the future
%   - guide for what hardware requirements are needed for given problems

\section*{Conclusion}

% sum up MR's importance
% setup on Janus, test, evaluate
% path for research to be conducted in future
% check if problems of Hadoop also exist in Spark (Data skew)
% restate project goals
% specific conditions of Janus
% add info to relatively new field

\bibliography{paper}
\bibliographystyle{plain}

\end{document}
