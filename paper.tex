\documentclass{article}
\usepackage{natbib}

%%%
%
% I wrote outline in the comments below to helpfully guide us
% but obviously feel free to modify/stray from it as you see fit
%
%%%

\title{High-Performance Data Analysis on Janus using Apache Spark}
\author{Nick Vanderweit \\
        Ning Gao \\
        Anitha Ganesha \\
        Michael Kasper}

\begin{document}
\maketitle

\section*{Introduction}
MapReduce \citep{dean-mapreduce} is a programming paradigm for performing highly
parallel tasks on large sets of data. Designed at Google to provide an
abstraction for interacting with raw data at scale, MapReduce is based on
a master/slave model, where the workers are assigned tasks as they become idle.

MapReduce is named after the two kinds of tasks that workers can be assigned.
A \emph{Map} is a user-defined function of type
$(k_1 \times v_1) \rightarrow list \: (k_2 \times v_2)$.
That is, it takes a key-value pair and produces a list of
intermediate key-value pairs. \emph{Reduce} has type
$(k_1 \times list \: v_1) \rightarrow list \: v_2$.\footnote{
These tasks are inspired by the \emph{map} and \emph{fold}
higher-order functions from functional programming, but are not identical.}
Reduce tasks are responsible for distilling a set of values associated with
a key to a smaller set, often containing just one value.

Despite its popularity, the MapReduce architecture has some significant
limitations \citep{lee-survey}. For one, Reduce tasks can only be assigned after
all Map tasks have been completed, meaning that nodes will idle even when there
is work remaining. Another major performance limitation, which will be central
to the motivation behind this project, is the architecture's reliance on a
distributed filesystem. This limits the performance of iterative computations
because every Map or Reduce task needs to read its input from storage and write
its output back.

\subsection*{Apache Spark}
Apache Spark \citep{zaharia} is designed to remove this bottleneck in order to
increase the performance of iterative computations. Spark was introduced by
AMPLab at University of California Berkeley. Its storage model is
based around \emph{Resilient Distributed Datasets}, or
RDDs \citep{zaharia_rdd}.

An RDD is processed in parallel by being split into \emph{partitions}, which
are shared between nodes. The dataset is accessed through a handle, which
represents the RDD as a \emph{lineage} of transformations on persistent data.
This allows RDDs to be recovered in the event of node failure, without writing
all intermediate data to distributed storage. Spark is written in Scala,
a statically-typed functional JVM language, and allows interaction with datasets
using the Scala collections API.

% what is map-reduce, history, applications (1 Ref Paper) 
%   - origin, date, creators, original purpose
%   - growing popularity, expanding applications
%   - common technologies/tools, Hadoop
% Janus description, why Janus can't use Hadoop (1 Ref Paper) 
%       \citep{tufo}
%   - what, where is it
%   - architecture, hardware: number of nodes, procs, storage
%   - focus on disk-less, common for HPC?
%   - has not yet employed MR technologies
%   - problem with Hadoop and disk-less

\subsection*{Janus Supercomputer}
The Janus supercomputer mainly consists of 1,368 computing nodes, with  12 cores
each that adds upto a total of 16,416 cores. Each node consists of two hex core
2.8Mhz Intel westmere processor. Each core contain 2GB of Ram with a total of
24GB of RAM per node. It contains 32TB high performance storage with lustre
filesystem \citep{tufo}. 

Since Janus is diskless, it is not possible to install Hadoop on Janus.
However, Spark is an in-memory based cluster computing system which does not
rely on disk data for every iteration and hence it could be easily deployed on
Janus supercomputer and evaluated.


% what is spark, differs from hadoop 
% Spark\citep{zaharia_rdd}, known for its performance in iterative and
% interactive tasks, was introduced by AmpLab group in University of California
% Berkeley. One of the best features that Spark supports is fault-tolerance.
% With hundreds or even thousands of machines in one cluster, the failure rate
% for one computer is not negligible. Spark is able to recover from failed
% machines by rebuilding the lost data from the lineage information of the RDD.
% On the other hand, Spark is so expressible that it not only supports mapreduce
% model and also Pregel, DryadLINQ, which means the program you run on mapreduce
% can be modified to work on Spark.
% Spark\citep{zaharia} \citep{zaharia_rdd}
%   - relationship with Scala (statically typed HL programming language for JVM)
%   - spark another MR solution, but all in-memory, year created
%   - solves the disk-less problem, can run on Janus
%   - other contrasts with Hadoop, preferred domains
%         - iterative, interactive, beyond MapReduce

%   - what is shark (do we want to use Shark in our project?)
%       - shark comes with its own set of benchmark queries

% Provides easy SQL programming interface to Spark's deep analysis tools.
% Shark\citep{engle}

\section*{Existing Performance Evaluations }
Despite being a relatively new technology, a considerable amount of performance
evaluations has been performed on the wide array of MapReduce solutions.
In Google's first paper introduction MapReduce, they provided a number of
example tasks in which MapReduce can be applied. A few of those listed were
word count, distributed grep, counting URL access frequency, \& reverse web-link
graph \citep{dean-mapreduce}. These examples have often been implemented for
benchmarking other MapReduce technologies and gives us a initial point in which
to start our evaluation of Spark on Janus.

%% Assuming Hadoop reference above
Hadoop, being one of the more popular MapReduce frameworks, has been in a
number of different studies. For this reason (and its large contrast in how
it utilizes memory and disk storage), Hadoop will make for an interesting
comparison with Spark.

% existing spark evaluation
Most of the benchmark programs run on Spark system were iterative and
interactive tasks. For instance, the initial work on RDDs \citep{zaharia_rdd}
used iterative machine learning algorithms, as well as PageRank with public
Wikipedia data. Based on the response time for interactive queries on
this data, Spark was shown to be over an order of magnitude faster than
on-disk technologies.
Andrew Pavlo et al. \cite{andrew} compared SQL performance between
MapReduce and DBMS system and Amplab \citep{amplab_bench}, inspired by the
benchmark in the previous paper and then conducted benchmarks on Shark,
Hive \citep{ashish}, Impala and Redshift.
% performance of mapreduce: an in-depth study
% written in Java
% Hadoop\citep{jiang}


As Spark is a newer technologies, it does not have the same quantity of
performance evaluations as Hadoop. But there still has been a significant
amount of conducted research on Spark (and the related Shark) that will gives
some initial insight and its performance that will aid us in our research.

% outperforms Hadoop by 10x in iterative machine learning jobs
% queries 39GB dataset with sub-second response time
% \citep{zaharia}

% Resilient Distributed Datasets (RDDs)
%     read only collection of objects
%     partitioned across a set of machines
%     data need not exist in physical storage

% Parallel Operations on these datasets
% \citep{zaharia}


%               "Performance and Scalability of Broadcast in Spark"
%               determined broadcast limited scalability
%               compares performance of Spark vs Hadoop on Logistic Reg prob
%               times during first and second iterations
%               ratio: computation vs broadcast times during an iteration
% Broadcast\citep{chowdhury}
% load into memory, read only happens once, overhead of creating workers reduce

As we will be running Spark with Shark, we will also briefly touch on the
reported performance of Shark. When compared to Hive (Hadoop's respective
SQL interface) we see that the run-times for the first iteration are comparable,
(when data need to be loaded into memory). But Shark is shown to be 10 orders
of magnitude faster during subsequent iterations of an algorithms \citep{engle}.
Even if this is not a pure comparison of the Hadoop and Spark technologies, we
can still glean some additional insights as to where each technology performs
best.


\section*{Project Proposal}
The main novelty of our project will be working with Spark on the Janus
supercomputer. Given the strength and popularity of MapReduce, adding this
additional functionality to Janus will be extremely beneficial for those using
Janus to conduct deep analysis on large datasets. Our project will consist of
the steps outlined in the following sections.

\subsection*{Installation}
The first phase of our project will be getting Spark installed and functioning
properly on Janus In addition to installing the core Spark framework, we will
also need to setup profiling tools. At this time, we are looking at employing
PerfSuite \citep{perfsuite}, Ganglia \citep{ganglia}, and YourKit
\citep{yourkit} to run our evaluations. However if the need arises we may
need to write some additional profiling scripts, if we cannot capture a metric
we desire, given the unique nature of the Spark framework.

\subsection*{Benchmarking}
Once Spark is properly setup on Janus, we will begin the project's second phase
of evaluating Spark's performance over a series of different tasks. First, we
will begin by implementing a distributed grep program, as described in
\citep{dean-mapreduce}. This will be a traditional single-iteration MapReduce
program. Next we will perform an iterative logistic regression program, which
we expect will showcase Sparks strength in iterative MapReduce problems.

As Spark implements an in-memory MapReduce solution, we additionally would like
to see what kind of performance hit Spark will take on Janus when the problem
size distributed to each worker is too large to completely fit in memory.

Finally, we are interested in seeing how well Spark handles load imbalance or
\emph{data skew}. There is an extension to Spark that controls data
partitioning of RDDs, which may address any concerns with data skew. We will
also be profiling Spark both with and without this extension.

Our dataset for most of our MapReduce tasks will be the Stack Overflow Data
Dump. This consist of all the data from the website's posts, comments,
votes, and other forum communication. There are in total 29 monthly data dumps,
where each month's dump averages 8GB. This will provide more than enough data
required for our analysis.

\subsection*{Project Expansion}
Our main focus will to successfully install Spark on Janus, and evaluate it's
performance on such an architecture. However, if time permits, we plan on
expanding our project by developing a clustering or classification program in
Spark, using the above mentioned dataset. Working on an application of a
non-trivial size will provide us with more information about how the Spark
setup could be improved on Janus.

\section*{Conclusion}
MapReduce has proved itself to be a important tool in performing deep analysis
on large datasets. It is our aim to bring this technology to the Janus
supercomputer by way of Apache Spark. After setting up Spark and its
extensions, we will profile the performance of its provided features to
determine if there are any issues specific to running on the Janus
architecture. The resulting benchmarks and analysis will hopefully serve as a
guide to those wanting to conduct research using MapReduce and is related
algorithms on Janus.

\bibliography{paper}
\bibliographystyle{plain}

\end{document}
